{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Kojack: Artistic Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import convert_kernel\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# What is this?\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Get pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pic_list = []\n",
    "\n",
    "for pic in os.listdir(\"../wiki_images/\"):\n",
    "    pic_list.append(pic)\n",
    "    jpgfile = Image.open(\"../wiki_images/\"+pic)\n",
    "    print jpgfile.bits, jpgfile.size, jpgfile.format\n",
    "\n",
    "print (pic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#synset = pd.read_csv('synset_words.txt', skipinitialspace=True, names = ['synset', 'words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Train Neural Network on Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Torch Code, need to convert to Keras Theano\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0     \n",
    "th feedforward_neural_doodle.lua   \n",
    "-model_name skip_noise_4   \n",
    "-masks_hdf5 data/starry/gen_doodles.hdf5   \n",
    "-batch_size 4   \n",
    "-num_mask_noise_times 0   \n",
    "-num_noise_channels 0   \n",
    "-learning_rate 1e-1   \n",
    "-half false  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Apply trained network to another picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Torch Code\n",
    "\n",
    "Stylize the doodle\n",
    "\n",
    "After the net is trained you can process any doodle with\n",
    "\n",
    "python apply.py --colors data/starry/gen_doodles.hdf5colors.npy --target_mask data/starry/style_mask.png --model data/out/starry_night.t7  \n",
    "  \n",
    "A pretrained starry_night net is there in pretrained folder. You can try it with\n",
    "\n",
    "python apply.py --colors pretrained/gen_doodles.hdf5colors.npy --target_mask data/starry/style_mask.png --model pretrained/starry_night.t7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Get Ling's code for taking a picture and recognizing it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 31675  100 31675    0     0   619k      0 --:--:-- --:--:-- --:--:--  736k\n"
     ]
    }
   ],
   "source": [
    "# !curl https://raw.githubusercontent.com/torch/tutorials/master/7_imagenet_classification/synset_words.txt -o synset_words.txt\n",
    "\n",
    "# install opencv if you haven't\n",
    "# conda install -c https://conda.binstar.org/menpo opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.utils.np_utils import convert_kernel\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import cv2, numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def VGG_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "synset = pd.read_csv('synset_words.txt', skipinitialspace=True, names = ['synset', 'words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_image():\n",
    "    retval, im = camera.read()\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "n02113023 Pembroke\n"
     ]
    }
   ],
   "source": [
    "CAMERA = 0\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "if CAMERA:\n",
    "    camera_port = 0\n",
    "    ramp_frames = 30\n",
    " \n",
    "    camera = cv2.VideoCapture(camera_port)\n",
    "\n",
    "\n",
    "    for i in xrange(ramp_frames):\n",
    "        temp = get_image()\n",
    "    camera_capture = get_image()\n",
    "    \n",
    "    im = cv2.resize(camera_capture, (224, 224)).astype(np.float32)\n",
    "    #file = \"/Users/lingqiangkong/repos/wip/vgg/test_image.png\"\n",
    "    #cv2.imwrite(file, camera_capture)\n",
    "    del (camera)\n",
    "else: \n",
    "    im = cv2.resize(cv2.imread('dog.jpg'), (224, 224)).astype(np.float32)\n",
    "\n",
    "im[:,:,0] -= 103.939\n",
    "im[:,:,1] -= 116.779\n",
    "im[:,:,2] -= 123.68\n",
    "im = im.transpose((2,0,1))\n",
    "im = np.expand_dims(im, axis=0)\n",
    "\n",
    "# Test pretrained model\n",
    "model = VGG_16('../data/vgg16_weights.h5')\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "out = model.predict(im)\n",
    "y_pred = np.argmax(out)\n",
    "print y_pred\n",
    "\n",
    "print synset.loc[y_pred].synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "usage: __main__.py [-h] [--nlabels NLABELS] [--style-image STYLE_IMAGE]\n",
      "                   [--style-mask STYLE_MASK] [--target-mask TARGET_MASK]\n",
      "                   [--content-image CONTENT_IMAGE]\n",
      "                   [--target-image-prefix TARGET_IMAGE_PREFIX]\n",
      "__main__.py: error: unrecognized arguments: -f /Users/KVASU/Library/Jupyter/runtime/kernel-2489f59b-c9e1-46d2-bdf3-775fe93dcc80.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KVASU/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "'''Neural doodle with Keras\n",
    "Script Usage:\n",
    "    # Arguments:\n",
    "    ```\n",
    "    --nlabels:              # of regions (colors) in mask images\n",
    "    --style-image:          image to learn style from\n",
    "    --style-mask:           semantic labels for style image\n",
    "    --target-mask:          semantic labels for target image (your doodle)\n",
    "    --content-image:        optional image to learn content from\n",
    "    --target-image-prefix:  path prefix for generated target images\n",
    "    ```\n",
    "    # Example 1: doodle using a style image, style mask\n",
    "    and target mask.\n",
    "    ```\n",
    "    python neural_doodle.py --nlabels 4 --style-image Monet/style.png \\\n",
    "    --style-mask Monet/style_mask.png --target-mask Monet/target_mask.png \\\n",
    "    --target-image-prefix generated/monet\n",
    "    ```\n",
    "    # Example 2: doodle using a style image, style mask,\n",
    "    target mask and an optional content image.\n",
    "    ```\n",
    "    python neural_doodle.py --nlabels 4 --style-image Renoir/style.png \\\n",
    "    --style-mask Renoir/style_mask.png --target-mask Renoir/target_mask.png \\\n",
    "    --content-image Renoir/creek.jpg \\\n",
    "    --target-image-prefix generated/renoir\n",
    "    ```\n",
    "References:\n",
    "[Dmitry Ulyanov's blog on fast-neural-doodle](http://dmitryulyanov.github.io/feed-forward-neural-doodle/)\n",
    "[Torch code for fast-neural-doodle](https://github.com/DmitryUlyanov/fast-neural-doodle)\n",
    "[Torch code for online-neural-doodle](https://github.com/DmitryUlyanov/online-neural-doodle)\n",
    "[Paper Texture Networks: Feed-forward Synthesis of Textures and Stylized Images](http://arxiv.org/abs/1603.03417)\n",
    "[Discussion on parameter tuning](https://github.com/fchollet/keras/issues/3705)\n",
    "Resources:\n",
    "Example images can be downloaded from\n",
    "https://github.com/DmitryUlyanov/fast-neural-doodle/tree/master/data\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imread, imsave\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import vgg19\n",
    "\n",
    "# Command line arguments\n",
    "parser = argparse.ArgumentParser(description='Keras neural doodle example')\n",
    "parser.add_argument('--nlabels', type=int,\n",
    "                    help='number of semantic labels'\n",
    "                    ' (regions in differnet colors)'\n",
    "                    ' in style_mask/target_mask')\n",
    "parser.add_argument('--style-image', type=str,\n",
    "                    help='path to image to learn style from')\n",
    "parser.add_argument('--style-mask', type=str,\n",
    "                    help='path to semantic mask of style image')\n",
    "parser.add_argument('--target-mask', type=str,\n",
    "                    help='path to semantic mask of target image')\n",
    "parser.add_argument('--content-image', type=str, default=None,\n",
    "                    help='path to optional content image')\n",
    "parser.add_argument('--target-image-prefix', type=str,\n",
    "                    help='path prefix for generated results')\n",
    "args = parser.parse_args()\n",
    "\n",
    "style_img_path = args.style_image\n",
    "style_mask_path = args.style_mask\n",
    "target_mask_path = args.target_mask\n",
    "content_img_path = args.content_image\n",
    "target_img_prefix = args.target_image_prefix\n",
    "use_content_img = content_img_path is not None\n",
    "\n",
    "nb_labels = args.nlabels\n",
    "nb_colors = 3  # RGB\n",
    "# determine image sizes based on target_mask\n",
    "ref_img = imread(target_mask_path)\n",
    "img_nrows, img_ncols = ref_img.shape[:2]\n",
    "\n",
    "total_variation_weight = 50.\n",
    "style_weight = 1.\n",
    "content_weight = 0.1 if use_content_img else 0\n",
    "\n",
    "content_feature_layers = ['block5_conv2']\n",
    "# To get better generation qualities, use more conv layers for style features\n",
    "style_feature_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1',\n",
    "                        'block4_conv1', 'block5_conv1']\n",
    "\n",
    "\n",
    "# helper functions for reading/processing images\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        x = x.reshape((3, img_nrows, img_ncols))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "def kmeans(xs, k):\n",
    "    assert xs.ndim == 2\n",
    "    try:\n",
    "        from sklearn.cluster import k_means\n",
    "        _, labels, _ = k_means(xs.astype(\"float64\"), k)\n",
    "    except ImportError:\n",
    "        from scipy.cluster.vq import kmeans2\n",
    "        _, labels = kmeans2(xs, k, missing='raise')\n",
    "    return labels\n",
    "\n",
    "\n",
    "def load_mask_labels():\n",
    "    '''Load both target and style masks.\n",
    "    A mask image (nr x nc) with m labels/colors will be loaded\n",
    "    as a 4D boolean tensor: (1, m, nr, nc) for 'th' or (1, nr, nc, m) for 'tf'\n",
    "    '''\n",
    "    target_mask_img = load_img(target_mask_path,\n",
    "                               target_size=(img_nrows, img_ncols))\n",
    "    target_mask_img = img_to_array(target_mask_img)\n",
    "    style_mask_img = load_img(style_mask_path,\n",
    "                              target_size=(img_nrows, img_ncols))\n",
    "    style_mask_img = img_to_array(style_mask_img)\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        mask_vecs = np.vstack([style_mask_img.reshape((3, -1)).T,\n",
    "                               target_mask_img.reshape((3, -1)).T])\n",
    "    else:\n",
    "        mask_vecs = np.vstack([style_mask_img.reshape((-1, 3)),\n",
    "                               target_mask_img.reshape((-1, 3))])\n",
    "\n",
    "    labels = kmeans(mask_vecs, nb_labels)\n",
    "    style_mask_label = labels[:img_nrows *\n",
    "                              img_ncols].reshape((img_nrows, img_ncols))\n",
    "    target_mask_label = labels[img_nrows *\n",
    "                               img_ncols:].reshape((img_nrows, img_ncols))\n",
    "\n",
    "    stack_axis = 0 if K.image_dim_ordering() == 'th' else -1\n",
    "    style_mask = np.stack([style_mask_label == r for r in xrange(nb_labels)],\n",
    "                          axis=stack_axis)\n",
    "    target_mask = np.stack([target_mask_label == r for r in xrange(nb_labels)],\n",
    "                           axis=stack_axis)\n",
    "\n",
    "    return (np.expand_dims(style_mask, axis=0),\n",
    "            np.expand_dims(target_mask, axis=0))\n",
    "\n",
    "# Create tensor variables for images\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    shape = (1, nb_colors, img_nrows, img_ncols)\n",
    "else:\n",
    "    shape = (1, img_nrows, img_ncols, nb_colors)\n",
    "\n",
    "style_image = K.variable(preprocess_image(style_img_path))\n",
    "target_image = K.placeholder(shape=shape)\n",
    "if use_content_img:\n",
    "    content_image = K.variable(preprocess_image(content_img_path))\n",
    "else:\n",
    "    content_image = K.zeros(shape=shape)\n",
    "\n",
    "images = K.concatenate([style_image, target_image, content_image], axis=0)\n",
    "\n",
    "# Create tensor variables for masks\n",
    "raw_style_mask, raw_target_mask = load_mask_labels()\n",
    "style_mask = K.variable(raw_style_mask.astype(\"float32\"))\n",
    "target_mask = K.variable(raw_target_mask.astype(\"float32\"))\n",
    "masks = K.concatenate([style_mask, target_mask], axis=0)\n",
    "\n",
    "# index constants for images and tasks variables\n",
    "STYLE, TARGET, CONTENT = 0, 1, 2\n",
    "\n",
    "# Build image model, mask model and use layer outputs as features\n",
    "# image model as VGG19\n",
    "image_model = vgg19.VGG19(include_top=False, input_tensor=images)\n",
    "\n",
    "# mask model as a series of pooling\n",
    "mask_input = Input(tensor=masks, shape=(None, None, None), name=\"mask_input\")\n",
    "x = mask_input\n",
    "for layer in image_model.layers[1:]:\n",
    "    name = 'mask_%s' % layer.name\n",
    "    if 'conv' in layer.name:\n",
    "        x = AveragePooling2D((3, 3), strides=(\n",
    "            1, 1), name=name, border_mode=\"same\")(x)\n",
    "    elif 'pool' in layer.name:\n",
    "        x = AveragePooling2D((2, 2), name=name)(x)\n",
    "mask_model = Model(mask_input, x)\n",
    "\n",
    "# Collect features from image_model and task_model\n",
    "image_features = {}\n",
    "mask_features = {}\n",
    "for img_layer, mask_layer in zip(image_model.layers, mask_model.layers):\n",
    "    if 'conv' in img_layer.name:\n",
    "        assert 'mask_' + img_layer.name == mask_layer.name\n",
    "        layer_name = img_layer.name\n",
    "        img_feat, mask_feat = img_layer.output, mask_layer.output\n",
    "        image_features[layer_name] = img_feat\n",
    "        mask_features[layer_name] = mask_feat\n",
    "\n",
    "\n",
    "# Define loss functions\n",
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    features = K.batch_flatten(x)\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "\n",
    "def region_style_loss(style_image, target_image, style_mask, target_mask):\n",
    "    '''Calculate style loss between style_image and target_image,\n",
    "    for one common region specified by their (boolean) masks\n",
    "    '''\n",
    "    assert 3 == K.ndim(style_image) == K.ndim(target_image)\n",
    "    assert 2 == K.ndim(style_mask) == K.ndim(target_mask)\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        masked_style = style_image * style_mask\n",
    "        masked_target = target_image * target_mask\n",
    "        nb_channels = K.shape(style_image)[0]\n",
    "    else:\n",
    "        masked_style = K.permute_dimensions(\n",
    "            style_image, (2, 0, 1)) * style_mask\n",
    "        masked_target = K.permute_dimensions(\n",
    "            target_image, (2, 0, 1)) * target_mask\n",
    "        nb_channels = K.shape(style_image)[-1]\n",
    "    s = gram_matrix(masked_style) / K.mean(style_mask) / nb_channels\n",
    "    c = gram_matrix(masked_target) / K.mean(target_mask) / nb_channels\n",
    "    return K.mean(K.square(s - c))\n",
    "\n",
    "\n",
    "def style_loss(style_image, target_image, style_masks, target_masks):\n",
    "    '''Calculate style loss between style_image and target_image,\n",
    "    in all regions.\n",
    "    '''\n",
    "    assert 3 == K.ndim(style_image) == K.ndim(target_image)\n",
    "    assert 3 == K.ndim(style_masks) == K.ndim(target_masks)\n",
    "    loss = K.variable(0)\n",
    "    for i in xrange(nb_labels):\n",
    "        if K.image_dim_ordering() == 'th':\n",
    "            style_mask = style_masks[i, :, :]\n",
    "            target_mask = target_masks[i, :, :]\n",
    "        else:\n",
    "            style_mask = style_masks[:, :, i]\n",
    "            target_mask = target_masks[:, :, i]\n",
    "        loss += region_style_loss(style_image,\n",
    "                                  target_image, style_mask, target_mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def content_loss(content_image, target_image):\n",
    "    return K.sum(K.square(target_image - content_image))\n",
    "\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    assert 4 == K.ndim(x)\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] -\n",
    "                     x[:, :, 1:, :img_ncols - 1])\n",
    "        b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] -\n",
    "                     x[:, :, :img_nrows - 1, 1:])\n",
    "    else:\n",
    "        a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] -\n",
    "                     x[:, 1:, :img_ncols - 1, :])\n",
    "        b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] -\n",
    "                     x[:, :img_nrows - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))\n",
    "\n",
    "# Overall loss is the weighted sum of content_loss, style_loss and tv_loss\n",
    "# Each individual loss uses features from image/mask models.\n",
    "loss = K.variable(0)\n",
    "for layer in content_feature_layers:\n",
    "    content_feat = image_features[layer][CONTENT, :, :, :]\n",
    "    target_feat = image_features[layer][TARGET, :, :, :]\n",
    "    loss += content_weight * content_loss(content_feat, target_feat)\n",
    "\n",
    "for layer in style_feature_layers:\n",
    "    style_feat = image_features[layer][STYLE, :, :, :]\n",
    "    target_feat = image_features[layer][TARGET, :, :, :]\n",
    "    style_masks = mask_features[layer][STYLE, :, :, :]\n",
    "    target_masks = mask_features[layer][TARGET, :, :, :]\n",
    "    sl = style_loss(style_feat, target_feat, style_masks, target_masks)\n",
    "    loss += (style_weight / len(style_feature_layers)) * sl\n",
    "\n",
    "loss += total_variation_weight * total_variation_loss(target_image)\n",
    "loss_grads = K.gradients(loss, target_image)\n",
    "\n",
    "# Evaluator class for computing efficiency\n",
    "outputs = [loss]\n",
    "if type(loss_grads) in {list, tuple}:\n",
    "    outputs += loss_grads\n",
    "else:\n",
    "    outputs.append(loss_grads)\n",
    "\n",
    "f_outputs = K.function([target_image], outputs)\n",
    "\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        x = x.reshape((1, 3, img_nrows, img_ncols))\n",
    "    else:\n",
    "        x = x.reshape((1, img_nrows, img_ncols, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    if len(outs[1:]) == 1:\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "    else:\n",
    "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "    return loss_value, grad_values\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator()\n",
    "\n",
    "# Generate images by iterative optimization\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    x = np.random.uniform(0, 255, (1, 3, img_nrows, img_ncols)) - 128.\n",
    "else:\n",
    "    x = np.random.uniform(0, 255, (1, img_nrows, img_ncols, 3)) - 128.\n",
    "\n",
    "for i in range(50):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
    "                                     fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    # save current generated image\n",
    "    img = deprocess_image(x.copy())\n",
    "    fname = target_img_prefix + '_at_iteration_%d.png' % i\n",
    "    imsave(fname, img)\n",
    "    end_time = time.time()\n",
    "    print('Image saved as', fname)\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
